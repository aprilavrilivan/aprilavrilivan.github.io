<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Fun With Diffusion Models!</title>
  <meta name="description" content="CS180/280A Project 5: Fun With Diffusion Models!" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    /* ------------------------------
       Theme & Reset
    ------------------------------ */
    :root{
      --bg: #0b0c10;
      --panel: #111318;
      --text: #e6e8ec;
      --muted: #a6adbb;
      --accent: #7c9cff;
      --accent-2: #4ad6a7;
      --bdr: #1e2230;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius-xl: 18px;
      --radius-lg: 14px;
      --radius-md: 12px;
      --radius-sm: 10px;
      --blur: saturate(120%) blur(0px);
    }
    [data-theme="light"]{
      --bg: #f6f7fb;
      --panel: #ffffff;
      --text: #0b1220;
      --muted: #5c6475;
      --accent: #3a66ff;
      --accent-2: #0dbb8e;
      --bdr: #e8ebf3;
      --shadow: 0 16px 40px rgba(16,24,40,.08);
    }
    * { box-sizing: border-box; }
    html, body { height: 100%; }
    body {
      margin: 0;
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
      background: radial-gradient(1200px 600px at 10% -10%, rgba(124,156,255,.12), transparent 60%),
                  radial-gradient(900px 500px at 100% 0%, rgba(74,214,167,.12), transparent 60%),
                  var(--bg);
      color: var(--text);
      line-height: 1.6;
    }

    /* ------------------------------
       Layout
    ------------------------------ */
    .wrap { max-width: 1400px; margin: 0 auto; padding: 32px 20px 80px; }
    header.site {
      position: sticky; top: 0; z-index: 50;
      backdrop-filter: var(--blur);
      background: color-mix(in oklab, var(--bg) 70%, transparent);
      border-bottom: 1px solid var(--bdr);
    }
    .site-inner { max-width: 1080px; margin: 0 auto; padding: 14px 20px; display: flex; align-items: center; gap: 12px; }
    .logo {
      width: 40px; height: 40px; display: grid; place-items:center; border-radius: 12px;
      background: linear-gradient(135deg, var(--accent), var(--accent-2));
      color: #fff; font-weight: 800;
      box-shadow: var(--shadow);
    }
    .brand { font-weight: 700; letter-spacing: .3px; }
    .grow { flex: 1; }

    .actions { display: flex; align-items: center; gap: 10px; }
    .btn, a.btn { appearance: none; border: 0; cursor: pointer; text-decoration: none; color: #fff; font-weight: 600; }
    .btn.primary { padding: 10px 14px; border-radius: 10px; background: linear-gradient(135deg, var(--accent), var(--accent-2)); box-shadow: var(--shadow); transition: transform .2s ease, box-shadow .2s ease, filter .2s ease; }
    .btn.primary:hover { transform: translateY(-1px); filter: saturate(110%); }
    .btn.ghost { padding: 10px 12px; border-radius: 10px; color: var(--text); background: transparent; border: 1px solid var(--bdr); }

    h1.page-title { font-size: clamp(22px, 2.4vw, 32px); margin: 18px 0 8px; }
    p.lead { color: var(--muted); margin: 0 0 22px; }

    /* ------------------------------
       Panels
    ------------------------------ */
    .panel { background: var(--panel); border: 1px solid var(--bdr); border-radius: var(--radius-xl); box-shadow: var(--shadow); overflow: hidden; }
    .panel .panel-hd { padding: 18px 20px; border-bottom: 1px solid var(--bdr); display: flex; align-items: center; justify-content: space-between; gap: 10px; }
    .panel .panel-hd h2 { margin: 0; font-size: 18px; }
    .panel .panel-bd { padding: 14px 16px 6px; }

    /* ------------------------------
       Tables ‚Üí Responsive Cards
    ------------------------------ */
    .tbl { width: 100%; border-collapse: separate; border-spacing: 0; }
    .tbl th, .tbl td { padding: 12px 14px; border-bottom: 1px solid var(--bdr); vertical-align: middle; }
    .tbl thead th { text-align: left; font-size: 13px; letter-spacing: .4px; color: var(--muted); text-transform: uppercase; }
    .tbl tbody tr { transition: background .2s ease, transform .08s ease; }
    .tbl tbody tr:hover { background: color-mix(in oklab, var(--panel) 85%, #000 15%); }
    .tbl img { width: 180px; height: auto; border-radius: 12px; display: block; border: 1px solid var(--bdr); box-shadow: var(--shadow); }

    /* Make it look like card list on small screens */
    @media (max-width: 760px){
      .tbl thead { display: none; }
      .tbl, .tbl tbody, .tbl tr, .tbl td { display: block; width: 100%; }
      .tbl tr { background: var(--panel); border: 1px solid var(--bdr); border-radius: var(--radius-lg); margin: 10px 0 16px; padding: 12px; }
      .tbl td { border: 0; padding: 8px 0; }
      .tbl td[data-label]::before { content: attr(data-label)"Ôºö"; display: inline-block; min-width: 9em; color: var(--muted); font-weight: 600; }
      .tbl img { width: 100%; }
    }

    /* Image zoom on hover */
    .thumb { position: relative; overflow: hidden; border-radius: 12px; }
    .thumb img { transform: scale(1); transition: transform .35s ease; }
    .thumb:hover img { transform: scale(1.04); }

    /* Tag pills for shifts */
    .pills { display: inline-flex; gap: 8px; flex-wrap: wrap; }
    .pill { display: inline-flex; align-items: center; gap: 6px; font-weight: 600; font-size: 12px; padding: 6px 10px; border-radius: 999px; border: 1px dashed var(--bdr); color: var(--text); background: color-mix(in oklab, var(--panel) 85%, #000 15%); }
    .pill .dot { width: 8px; height: 8px; border-radius: 999px; background: var(--accent); box-shadow: 0 0 0 2px color-mix(in oklab, var(--accent) 20%, transparent); }
    .pill.red .dot { background: #ff6b6b; }
    .pill.green .dot { background: #4ad6a7; }

    /* Footer */
    footer { margin: 28px 0; display: flex; align-items: center; gap: 12px; }
    .link { color: #fff; text-decoration: none; }

    /* Back button */
    .back-btn { display: inline-flex; align-items: center; gap: 8px; font-weight: 700; padding: 12px 16px; border-radius: 12px; border: 1px solid var(--bdr); background: linear-gradient(180deg, color-mix(in oklab, var(--panel) 90%, #fff 10%), var(--panel)); text-decoration: none; color: var(--text); box-shadow: var(--shadow); transition: transform .2s ease, background .2s ease; }
    .back-btn:hover { transform: translateY(-1px); }

    /* Utility */
    .muted { color: var(--muted); }
    .right { text-align: right; }

    /* Mini step box */
    .mini-box {
      margin: 12px 4px 8px;
      background: color-mix(in oklab, var(--panel) 92%, #000 8%);
      border: 1px dashed var(--bdr);
      border-radius: 12px;
      box-shadow: var(--shadow);
      padding: 12px 14px;
    }
    .mini-box .mini-hd{
      display:flex; align-items:center; justify-content:space-between; gap:10px;
      font-weight:700; font-size:14px;
    }
    .mini-box .mini-hd .hint{ color: var(--muted); font-weight:600; font-size:20px; }
    .mini-box .mini-bd{ margin-top:10px; color: var(--text); }
    .mini-box ol{ margin: 6px 0 0 18px; padding:0; }
    .mini-box li{ margin: 4px 0; line-height: 1.5; }
    .mini-toggle{
      appearance:none; border:1px solid var(--bdr); background:transparent; color:var(--text);
      font-weight:600; border-radius:10px; padding:6px 10px; cursor:pointer;
    }
    .mini-toggle:hover{ filter:saturate(110%); transform: translateY(-1px); }
    @media (max-width:760px){ .mini-box{ padding: 10px 12px; } }

    
    .result-box {
      margin: 10px 0;
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
    }
    .result-box .thumb {
      flex: 1 1 200px;
      max-width: 300px;
      border-radius: 12px;
      overflow: hidden;
      border: 1px solid var(--bdr);
      box-shadow: var(--shadow);
    }
    .result-box img {
      width: 100%;
      height: auto;
      display: block;
    }
    /* ÂçïÁã¨ÊîæÂ§ß main part ÈÇ£‰∏ÄÂº† */
    .result-box .large-box {
      flex: 1 1 450px;  /* Ë∞ÉÊï¥Âü∫Á°ÄÂÆΩÂ∫¶ */
      max-width: 650px; /* ËÆæÁΩÆÊúÄÂ§ßÂÆΩÂ∫¶ */
    }

      .result-line {
      display: flex;
      justify-content: space-between; /* Êàñ center */
      gap: 12px;
      flex-wrap: nowrap; /* Âº∫Âà∂‰∏çÊç¢Ë°å */
    }
    .result-line .thumb {
      flex: 1 1 0;
      max-width: 180px; /* ÊéßÂà∂ÂçïÂº†ÂõæÂÆΩÂ∫¶ */
    }
    .result-line img {
      width: 100%;
      height: auto;
      display: block;
    }
    
    .caption {
      text-align: center;
      font-size: 13px;
      color: var(--muted);
      margin-top: 6px;
    }
  </style>
</head>
    
<body>
  <!-- Sticky Header -->
  <header class="site">
    <div class="site-inner">
      <div class="logo" aria-hidden="true">P2</div>
      <div class="brand">CS180/280A ¬∑ Project 5</div>
      <div class="grow"></div>
      <div class="actions">
        <button class="btn ghost" id="themeToggle" aria-label="Switch Theme">üåì Theme</button>
        <a class="btn primary" href="../index.html">back to main</a>
      </div>
    </div>
  </header>

  <main class="wrap">
    <h1 class="page-title">Fun With Diffusion Models!</h1>
    <p class="lead">This project aims to explore Diffusion Models</span>.</p>
    
    <!-- P1 -->
    <section class="panel" aria-labelledby="sec-simple">
      <div class="panel-hd">
        <h2 id="sec-simple">Part 0: Setup</h2>
        <span class="muted">4 sections in total</span>
      </div>
      <div class="panel-bd">
        <!-- Small step box -->
        <div class="mini-box" id="0.1">
          <div class="mini-hd">
            <span>Part 0</span>
            <span class="hint">0.1</span>
            <button class="mini-toggle" type="button" data-target="#0.1-bo">Hide</button>
          </div>
          <div class="mini-bd" id="0.1-body">
            <ol>
              <li><p>This part aims to play with DeepFloyd, which is a two stage diffusion model trained by Stability AI. The first step
                  I did is to create some imaginative prompts, and encode them to vectors using T5 Text Encoder Embeddings Generator. 
                  I created 11 prompts, as shown below in the generator's website:</p>
                  <div class="result-box">
                    <div class="thumb large-box">
                      <img src="media/questions.png" alt="Box filter result 1">
                      <div class="caption">the prompts</div>
                    </div>
                  </div>
                </li>
              <li>
                <p>Then I fed the embedded vectors to DeepFloyd, with random seed = 100. The following are three sets of results</p>
                <p>--------------------------------------------------------------------------------------------------------------------------</p>
                <p>Prompt: "a wide angle photo of the campanile reflected in a rain puddle at night"</p>
                <p>Reflect: The three pictures are all clear, and they all matches the key words in prompt: wide angle/campanile/reflect/rain puddle/night. As the inference 
                  steps grow, the pictures gradually contain more details (because the model can diffuse more noise), and the angles of camera becomes wider.</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/prompt1_20.png" alt="Box filter result 1">
                      <div class="caption">20 num_inference_steps</div>
                    </div>
                    <div class="thumb">
                      <img src="media/prompt1_40.png" alt="Box filter result 1">
                      <div class="caption">40 num_inference_steps</div>
                    </div>
                    <div class="thumb">
                      <img src="media/prompt1_80.png" alt="Box filter result 1">
                      <div class="caption">80 num_inference_steps</div>
                    </div>
                  </div>
                <p>Prompt: "a watercolor painting of robots studying under paper lanterns in a quiet library"</p>
                <p>Reflect: All three pictures didn't manage to obey all the instructions in the prompt: Only the third picture captures the detail 
                  "paper lanters", and only the second image captures the detail "library", but it doesn't show the keyword "studying". The robots tend to have 
                  wrong number of legs, and their hands can't be characterized well.
                  But anyway, at least all three pictures follow the instruction of "watercolor painting" and "robotics", and overall, the images are clear, with more details added
                  as the inference steps grow, which aligns the observation in the first set of images.</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/prompt2_20.png" alt="Box filter result 1">
                      <div class="caption">20 num_inference_steps</div>
                    </div>
                    <div class="thumb">
                      <img src="media/prompt2_40.png" alt="Box filter result 1">
                      <div class="caption">40 num_inference_steps</div>
                    </div>
                    <div class="thumb">
                      <img src="media/prompt2_80.png" alt="Box filter result 1">
                      <div class="caption">80 num_inference_steps</div>
                    </div>
                  </div>
                <p>Prompt: "an aerial photograph of terraced rice fields shaped like a spiral galaxy"</p>
                <p>Reflect: The terraced rice fields are more and more shaped like a spiral galaxy as the inference steps grow, indicating that the model could better understand the semantic in the prompt as the inference time becomes longer. Overall, the three
                    images all obeys the instructions in the prompt, and are all clear without distinct noise„ÄÇ</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/prompt3_20.png" alt="Box filter result 1">
                      <div class="caption">20 num_inference_steps</div>
                    </div>
                    <div class="thumb">
                      <img src="media/prompt3_40.png" alt="Box filter result 1">
                      <div class="caption">40 num_inference_steps</div>
                    </div>
                    <div class="thumb">
                      <img src="media/prompt3_80.png" alt="Box filter result 1">
                      <div class="caption">80 num_inference_steps</div>
                    </div>
                  </div>
              </li>
            </ol>
          </div>
        </div>
    </section>
    <!---------------- P2 ----------->
    <section class="panel" aria-labelledby="sec-simple">
      <div class="panel-hd">
        <h2 id="sec-simple">Part 1: Sampling Loops</h2>
        <span class="muted">9 section in total</span>
      </div>
      <div class="panel-bd">
        <!-- Small step box -->
        <div class="mini-box" id="1.1">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.1</span>
            <button class="mini-toggle" type="button" data-target="#1.1-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.1-body">
            <ol>
              <li><p>The following demonstrates a simple fwd proc of diffusion sampling</p>
                  <p>Here is the critical part of the forward(im, t) function: choose an alpha, sqrt the alpha
                    , generate a pure noise img, and merge the noise img and the orignal img in a weighted way according to alpha.</p>
                  <div class="result-box">
                    <div class="thumb large-box">
                      <img src="media/forward_proc.png" alt="Box filter result 1">
                      <div class="caption">the forward process</div>
                    </div>
                  </div>
              </li>
              <li>
                <p>
                  Here are the Campanile at noise level [250, 500, 750]:
                </p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/0.png" alt="Box filter result 1">
                      <div class="caption">Berkeley Campanile</div>
                    </div>
                </div>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/250.png" alt="Box filter result 1">
                      <div class="caption">Noisy Campanile at t=250</div>
                    </div>
                    <div class="thumb">
                      <img src="media/500.png" alt="Box filter result 1">
                      <div class="caption">Noisy Campanile at t=500</div>
                    </div>
                    <div class="thumb">
                      <img src="media/750.png" alt="Box filter result 1">
                      <div class="caption">Noisy Campanile at t=750</div>
                    </div>
                  </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.2">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.2</span>
            <button class="mini-toggle" type="button" data-target="#1.2-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.2-body">
            <ol>
              <li><p>Using classical method (i.e. Gaussian Blurring) to denoise the above images is hard. I tried my best to denoise them
                    by increasing the Gaussian kernal size w.r.t larger noise step, but it's still hard to identify the campanile when 
                    noise step gets larger.</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/blur250.png" alt="Box filter result 1">
                      <div class="caption">Gaussian Blur Denoising at t=250</div>
                    </div>
                    <div class="thumb">
                      <img src="media/blur500.png" alt="Box filter result 1">
                      <div class="caption">Gaussian Blur Denoising at t=500</div>
                    </div>
                    <div class="thumb">
                      <img src="media/blur750.png" alt="Box filter result 1">
                      <div class="caption">Gaussian Blur Denoising at t=250</div>
                    </div>
                  </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.3">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.3</span>
            <button class="mini-toggle" type="button" data-target="#1.3-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.3-body">
            <ol>
              <li><p>It would be a better choice to use One-Step Denoising by the stage_1.unet of the DeepFloyd model. The model takes in the 
                    [noise image, prompt "a high quality photo", noise step t], and outputs the estimated one-step noise e. We can use the reverse
                     formula to get clean x_0:</p>
                  <div class="result-box">
                    <div class="thumb">
                      <img src="media/reverseFormula.png" alt="Box filter result 1">
                    </div>
                  </div>
                <p>Here are the results:</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/deno250.png" alt="Box filter result 1">
                      <div class="caption">One-Step Denoised Campanile at t=250</div>
                    </div>
                    <div class="thumb">
                      <img src="media/deno500.png" alt="Box filter result 1">
                      <div class="caption">One-Step Denoised Campanile at t=500</div>
                    </div>
                    <div class="thumb">
                      <img src="media/deno750.png" alt="Box filter result 1">
                      <div class="caption">One-Step Denoised Campanile at t=750</div>
                    </div>
                  </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.4">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.4</span>
            <button class="mini-toggle" type="button" data-target="#1.4-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.4-body">
            <ol>
              <li><p>To achieve even better denoising outcome, we can use iterative denoising. We can iteratively denoise from t=990 to t=0, 
                      with a stride=30. To do this, first I implemented a strided_timesteps list, which contains timesteps like 990,960,930...,60,30,0, and 
                      also initialize the timesteps to stage_1 model using the function stage_1.scheduler.set_timesteps(timesteps=strided_timesteps):</p>
                  <div class="result-box">
                    <div class="thumb large-box">
                      <img src="media/createStride.png" alt="Box filter result 1">
                    </div>
                  </div>
                </li>
              <li>
                <p>Then I implemented the iterative_denoise function, which translates the following iterative denoise formula:</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/iterativeFormula.png" alt="Box filter result 1">
                    </div>
                </div>
                <p>to code:</p>
                  <div class="result-box">
                    <div class="thumb large-box">
                      <img src="media/getAlpha.png" alt="Box filter result 1">
                    </div>
                    <div class="thumb large-box">
                      <img src="media/compFormula.png" alt="Box filter result 1">
                    </div>
                  </div>
              </li>
              <li>
                <p>After that, I tried to add noise to the Campanilie picture to timestep[10], which is equivalent to t=690, and denoise back
                with stride=30 till t=0. I saved the progress every 5 timestep (i.e. every t=150 steps). I compared the final result
                  with one-step denoising and Gaussian Blurring, finding the iterative denoise versions achieves the best result.</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/90.png" alt="Box filter result 1">
                      <div class="caption"> Noisy Campanile at t=90</div>
                    </div>
                    <div class="thumb">
                      <img src="media/240.png" alt="Box filter result 1">
                      <div class="caption">Noisy Campanile at t=240</div>
                    </div>
                    <div class="thumb">
                      <img src="media/390.png" alt="Box filter result 1">
                      <div class="caption">Noisy Campanile at t=390</div>
                    </div>
                    <div class="thumb">
                      <img src="media/540.png" alt="Box filter result 1">
                      <div class="caption"> Noisy Campanile at t=540</div>
                    </div>
                    <div class="thumb">
                      <img src="media/690.png" alt="Box filter result 1">
                      <div class="caption">Noisy Campanile at t=690</div>
                    </div>
                  </div>
                  <div class="result-box">
                    <div class="thumb">
                      <img src="media/original.png" alt="Box filter result 1">
                      <div class="caption"> original Campanilie </div>
                    </div>
                    <div class="thumb">
                      <img src="media/iterative_deno.png" alt="Box filter result 1">
                      <div class="caption">Iteratively Denoised Campanile</div>
                    </div>
                    <div class="thumb">
                      <img src="media/onestep_deno.png" alt="Box filter result 1">
                      <div class="caption">One-Step Denoised Campanile</div>
                    </div>
                    <div class="thumb">
                      <img src="media/gaus_deno.png" alt="Box filter result 1">
                      <div class="caption">Gaussian Blurred Campanile</div>
                    </div>
                  </div>
              </li>
            </ol>
          </div>
        </div>
      <!-- Small step box -->
        <div class="mini-box" id="1.5">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.5</span>
            <button class="mini-toggle" type="button" data-target="#1.5-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.5-body">
            <ol>
              <li><p>To generate arbitarily denoised images, we could add noise to the Campanile image to time t=1000 (let i_start = 0), which tranforms it into a pure noise image. Then we perform the same 
              iteratively denoise process till time =0, and see what kind of clean images the model would output:</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/sample1.png" alt="Box filter result 1">
                    <div class="caption">sample1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/sample2.png" alt="Box filter result 1">
                    <div class="caption">sample2</div>
                  </div>
                  <div class="thumb">
                    <img src="media/sample3.png" alt="Box filter result 1">
                    <div class="caption">sample3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/sample4.png" alt="Box filter result 1">
                    <div class="caption">sample4</div>
                  </div>
                  <div class="thumb">
                    <img src="media/sample5.png" alt="Box filter result 1">
                    <div class="caption">sample5</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.6">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.6</span>
            <button class="mini-toggle" type="button" data-target="#1.6-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.6-body">
            <ol>
              <li><p>To make the above randomly sampled images perform higher quality, we can adopt the CFG trick: which is that at each denoising stage, 
              we run the model twice, one with prompt "a high quality photo", another with null prompt. Then we do weighted average to the output noise and 
              run a strided denoising step. Here are the critical code snippets in the function iterative_denoise_cfg:</p>
              <div class="result-box">
                <div class="thumb large-box">
                    <img src="media/CFGCode.png" alt="Box filter result 1">
                    <div class="caption">code snippets of weighted averaging noise</div>
                  </div>
              </div>
              <p>Here are five sampled diffution output images using CFG. The quality is much higher than that in the previous srection.</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/CFG1.png" alt="Box filter result 1">
                    <div class="caption">sample1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/CFG2.png" alt="Box filter result 1">
                    <div class="caption">sample2</div>
                  </div>
                  <div class="thumb">
                    <img src="media/CFG3.png" alt="Box filter result 1">
                    <div class="caption">sample3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/CFG4.png" alt="Box filter result 1">
                    <div class="caption">sample4</div>
                  </div>
                  <div class="thumb">
                    <img src="media/CFG5.png" alt="Box filter result 1">
                    <div class="caption">sample5</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.7.0">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.7.0</span>
            <button class="mini-toggle" type="button" data-target="#1.7.0-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.7.0-body">
            <ol>
              <li><p>We know that the model tends to be more "hallucinative" when we add more noise to the original picture and use the model to 
              denoise it back. The following are edits of the Campanile image, using the given prompt at noise levels [1, 3, 5, 7, 10, 20] with the conditional text prompt "a high quality photo":</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/campanile1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/campanile3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/campanile5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/campanile7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/campanile10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/campanile20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/original.png" alt="Box filter result 1">
                    <div class="caption">original companile</div>
                  </div>
                </div>
              </li>
              <li><p>I also tested this on two of my own pictures:</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/mypic1_1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic1_3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic1_5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic1_7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic1_10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/mypic1_20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic1_original.png" alt="Box filter result 1">
                    <div class="caption">original companile</div>
                  </div>
                </div>
                <div class="result-box">
                  <div class="thumb">
                    <img src="media/mypic2_1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic2_3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic2_5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic2_7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic2_10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/mypic2_20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mypic2_original.png" alt="Box filter result 1">
                    <div class="caption">original companile</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
      <!-- Small step box -->
        <div class="mini-box" id="1.7.1">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.7.1</span>
            <button class="mini-toggle" type="button" data-target="#1.7.1-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.7.1-body">
            <ol>
              <li><p>When we start with some nonrealistic images (e.g. painting, a sketch, some scribbles), the above pipeline
                tends to project it onto the natural image manifold. For example, the following pixel-style face from the web:</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/face1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/face3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/face5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/face7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/face10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/face20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/faceOriginal.png" alt="Box filter result 1">
                    <div class="caption">original web pic</div>
                  </div>
                </div>
              </li>
              <li><p>I also tested this on two pictures drawn by myself:</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/stickman1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/stickman3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/stickman5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/stickman7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/stickman10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/stickman20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/stickmanOriginal.png" alt="Box filter result 1">
                    <div class="caption">original stickman</div>
                  </div>
                </div>
                <div class="result-box">
                  <div class="thumb">
                    <img src="media/gem1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/gem3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/gem5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/gem7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/gem10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/gem20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/gemOriginal.png" alt="Box filter result 1">
                    <div class="caption">original germ</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
      <!-- Small step box -->
        <div class="mini-box" id="1.7.2">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.7.2</span>
            <button class="mini-toggle" type="button" data-target="#1.7.2-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.7.2-body">
            <ol>
              <li><p>We can also use this process to do inpainting. That is, given a mask, the diffusion process only edits the area where the mask is 1. To realize this, at every diffusing step
              when we get the result x_t, we only keep the unmasked area, and replace the masked area with the original corresponding image at that stage. The following is the critical 
              part of the inpainting function:</p>
              <div class="result-box">
                  <div class="thumb large-box">
                    <img src="media/codeInpainting.png" alt="Box filter result 1">
                  </div>
              </div>
              </li>
              <li>
              <p>Here I first inpainted the Companile using my own mask:</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/original.png" alt="Box filter result 1">
                    <div class="caption">Orginal Companile</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mask1.png" alt="Box filter result 1">
                    <div class="caption">The mask</div>
                  </div>
                  <div class="thumb">
                    <img src="media/holeToFill.png" alt="Box filter result 1">
                    <div class="caption">Area to replace</div>
                  </div>
                  <div class="thumb">
                    <img src="media/inpaintedCampanile.png" alt="Box filter result 1">
                    <div class="caption">The inpainted picture</div>
                  </div>
                </div>
              </li>
              <li><p>I also tested this on two pictures drawn by myself:</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/airplaneOriginal.png" alt="Box filter result 1">
                    <div class="caption">Orginal Airplane</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mask2.png" alt="Box filter result 1">
                    <div class="caption">The mask</div>
                  </div>
                  <div class="thumb">
                    <img src="media/toreplace2.png" alt="Box filter result 1">
                    <div class="caption">Area to replace</div>
                  </div>
                  <div class="thumb">
                    <img src="media/airplainInpainted.png" alt="Box filter result 1">
                    <div class="caption">The inpainted picture</div>
                  </div>
              </div>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/railwayOriginal.png" alt="Box filter result 1">
                    <div class="caption">Orginal Companile</div>
                  </div>
                  <div class="thumb">
                    <img src="media/mask3.png" alt="Box filter result 1">
                    <div class="caption">The mask</div>
                  </div>
                  <div class="thumb">
                    <img src="media/toreplace3.png" alt="Box filter result 1">
                    <div class="caption">Area to replace</div>
                  </div>
                  <div class="thumb">
                    <img src="media/railwayInpainted.png" alt="Box filter result 1">
                    <div class="caption">The inpainted picture</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.7.3">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.7.3</span>
            <button class="mini-toggle" type="button" data-target="#1.7.3-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.7.3-body">
            <ol>
              <li><p>Now, we will do the same thing as the previous section, but guide the projection with a text prompt.</p>
              <p>Here is the version of berkeley Campanile with the prompt:"a wide angle photo of the campanile reflected in a rain puddle at night":</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/prompt1_1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt1_3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt1_5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt1_7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt1_10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/prompt1_21.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/original.png" alt="Box filter result 1">
                    <div class="caption">original campanile</div>
                  </div>
                </div>
              </li>
              <li><p>I also tested this on two pictures drawn by myself:</p>
                <p>Prompt: "a watercolor painting of robots studying under paper lanterns in a quiet library"</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/prompt2_1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt2_3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt2_5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt2_7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt2_10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/prompt2_21.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/prompt2Original.png" alt="Box filter result 1">
                    <div class="caption">original robot</div>
                  </div>
                </div>
                <p>Prompt: "an aerial photograph of terraced rice fields shaped like a spiral galaxy"</p>
                <div class="result-box">
                  <div class="thumb">
                    <img src="media/galaxy1.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=1</div>
                  </div>
                  <div class="thumb">
                    <img src="media/galaxy3.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=3</div>
                  </div>
                  <div class="thumb">
                    <img src="media/galaxy5.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=5</div>
                  </div>
                  <div class="thumb">
                    <img src="media/galaxy7.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=7</div>
                  </div>
                  <div class="thumb">
                    <img src="media/galaxy10.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=10</div>
                  </div>                  
                  <div class="thumb">
                    <img src="media/galaxy20.png" alt="Box filter result 1">
                    <div class="caption">SDEdit with i_start=20</div>
                  </div>
                  <div class="thumb">
                    <img src="media/galaxyOriginal.png" alt="Box filter result 1">
                    <div class="caption">original galaxy</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="1.8">
          <div class="mini-hd">
            <span>Part 1</span>
            <span class="hint">1.8</span>
            <button class="mini-toggle" type="button" data-target="#1.8-bo">Hide</button>
          </div>
          <div class="mini-bd" id="1.8-body">
            <ol>
              <li><p>Then, I implemented a visual anagram using diffusion. The picture describes different scenes when fliped upsidedown. To do this, I did two CFGs in one timestep, one for upright picture and one for upside uown picture,
                and then average them together</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/anacode1.png" alt="Box filter result 1">
                    <div class="caption">CFG for upright</div>
                  </div>
                  <div class="thumb">
                    <img src="media/anacode2.png" alt="Box filter result 1">
                    <div class="caption">CFG for upsidedown</div>
                  </div>
                  <div class="thumb">
                    <img src="media/anacode3.png" alt="Box filter result 1">
                    <div class="caption">merge the noise together</div>
                  </div>
                </div>
              </li>
              <li>
                <p>p1 = "an oil painting of paper lanterns floating in a circle on a lake at night"</p>
                <p>p2 = "an oil painting of a dragon's face made of glowing scales in the dark"</p>
              <div class="result-box">
                  <div class="thumb">
                    <img src="media/anacode111.png" alt="Box filter result 1">
                    <div class="caption">upright</div>
                  </div>
                  <div class="thumb">
                    <img src="media/anacode2222.png" alt="Box filter result 1">
                    <div class="caption">upsidedown</div>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
    </section>   
          
    <!---------------- P3 ----------->
    <section class="panel" aria-labelledby="sec-simple">
      <div class="panel-hd">
        <h2 id="sec-simple">Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
        <span class="muted">1 section in total</span>
      </div>
      <div class="panel-bd">
        <!-- Small step box -->
        <div class="mini-box" id="2.1ÔΩû2.3">
          <div class="mini-hd">
            <span>Part 2.1ÔΩû2.3</span>
            <span class="hint">2.1ÔΩû2.3</span>
            <button class="mini-toggle" type="button" data-target="#2.1ÔΩû2.3-bo">Hide</button>
          </div>
          <div class="mini-bd" id="2.1ÔΩû2.3-body">
            <ol>
              <li><p>In this section, I managed to load data from Lego scene provided in the course website, and 
                  plot the cameras, rays, and samples in 3D using Viser.</p>
                  <p>To achieve this , firstly I wrote a function transform(c2w, x_c) to transform camera coordinate to 
                      world coordinate. Then I wrote a function pixel_to_camera(K, uv, s) to transform pixel coordinates
                      to camera coordinates, given arbitary depth s. Finally I wrote a function pixel_to_ray(K, c2w, uv, depth)
                      to get the unit ray that shoot from a certain pixel of the camera. The detailed implementation is shown below.</p>
                  <div class="result-box">
                    <div class="thumb">
                      <img src="media/transform.png" alt="Box filter result 1">
                      <div class="caption">transform(c2w, x_c)</div>
                    </div>
                    <div class="thumb ">
                      <img src="media/pixel2camera.png" alt="Box filter result 1">
                      <div class="caption">pixel_to_camera(K, uv, s) </div>
                    </div>
                    <div class="thumb">
                      <img src="media/pixel2ray.png" alt="Box filter result 1">
                      <div class="caption">pixel_to_ray(K, c2w, uv, depth)</div>
                    </div>
                  </div>
              </li>
              <li><p>I used the second method mentioned in the official website, which randomly pick N rays and sample along them
                      finally return the world coordinates of those points. The detailed procedure is shown in the picture below</p> 
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/samplealongrays.png" alt="Box filter result 1">
                      <div class="caption">Sample along rays function</div>
                    </div>
                  </div>
              </li>
              <li>
                <p>After doing the above procedure, I integrate the above tools to a RaysDataset class.
                  This RaysDataset class precomputes all rays for every training image so we can sample them efficiently
                  during NeRF training. It builds a pixel grid, converts each pixel into a ray using the camera intrinsics 
                  and extrinsics, and stores the ray origins, directions, and corresponding RGB values in flattened form. 
                  During training, we can randomly draw a batch of rays from the entire dataset or from a specific camera for debugging.
                  This makes the dataloader simple, fast, and convenient for both global sampling and visualization in Part 2.3.</p>
                <p>Then, I implemented a visualization function visualize_random_rays(dataset, K, num_rays, n_samples=) that helps visualize
                   the dataset using Viser, following the code structure provided in the official website. These are two of the visualized results:</p>
                <div class="result-box">
                    <div class="thumb large-box">
                      <img src="media/Viser3.png" alt="Box filter result 1">
                      <div class="caption">Visualization with random rays</div>
                    </div>
                    <div class="thumb large-box">
                      <img src="media/from_single.png" alt="Box filter result 1">
                      <div class="caption">Visualization with rays from a single camera</div>
                    </div>
                  </div>
              </li>
            </ol>
          </div>
        </div>
        <!-- Small step box -->
        <div class="mini-box" id="2.4ÔΩû2.5">
          <div class="mini-hd">
            <span>Part 2.4ÔΩû2.5</span>
            <span class="hint">2.4ÔΩû2.5</span>
            <button class="mini-toggle" type="button" data-target="#2.4ÔΩû2.5-bo">Hide</button>
          </div>
          <div class="mini-bd" id="2.4ÔΩû2.5-body">
            <ol>
              <li><p>In this section, I managed to train a Nerf using the lego dataset.</p>
                  <p>To achieve this , firstly I implemented the nerf model.the NeRF model follows the standard 
                    design proposed in the original paper. I apply sinusoidal positional encoding to both 3D positions
                    and view directions, using higher-frequency bands for positions and lower-frequency ones for 
                    directions. The network is built as an eight-layer MLP with a skip connection at the middle layer,
                    which helps preserve high-frequency details during training. The density (sigma) and color branches 
                    are separated: sigma is predicted directly from the spatial features, while RGB is produced by 
                    combining learned features with the encoded viewing direction. 
                    A final sigmoid ensures valid color outputs.</p>
              </li>
              <li><p>To train the NeRF model, I implemented the volume rendering function as indeicated in the instruction. Then,
                I randomly sample a large batch of rays at each iteration and generate points 
                along them with stratified sampling. The network is optimized using Adam with a learning rate of 5 \times 10^{-4}, 
                batch size 8192, and 64 samples per ray. I trained for 5000 iterations on Google Colab T4 GPU.
                During training, I rendered a validation image every 250 iterations to track progress. PSNR is used as the main evaluation metric, and I log both training loss and validation PSNR to
                monitor convergence. This setup provides stable optimization and reproduces the expected NeRF performance on the 
                Lego dataset. Here are the rendering results during training:</p> 
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/val_iter_000001.png" alt="Box filter result 1">
                      <div class="caption">the first iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_000250.png" alt="Box filter result 1">
                      <div class="caption">the 250th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_000750.png" alt="Box filter result 1">
                      <div class="caption">the 750th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_001500.png" alt="Box filter result 1">
                      <div class="caption">the 1500th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_005000.png" alt="Box filter result 1">
                      <div class="caption">the 5000th iteration</div>
                    </div>
                  </div>
                <p>And here is the PSNR curve:</p>
                <div class="result-box">
                  <div class="thumb large-box">
                      <img src="media/psnr_curve.png" alt="Box filter result 1">
                      <div class="caption">the PSNR curve during trainig</div>
                    </div>
                </div>
                <p>Finally, here is the GIF of camera circling the lego car</p>
                <div class="result-box">
                  <div class="thumb large-box">
                      <img src="media/output.gif" alt="Box filter result 1">
                      <div class="caption">Spherical rendering video of the Lego</div>
                    </div>
                </div>
              </li>
              <li>
                <p>Then I tested using my own dataset. I found that near=0.3 far=0.7 is suitable for my dataset. 
                  I set n_samples=64, batch_size=4096, max_iters=5000, lr=5e-4, and the rest of the hyperparameters are the same 
                  as above.During implementation, I found that the model failed to converge when trained on my first 
                  set of images. The PSNR stayed extremely low and the rendered results were almost blank. 
                  After debugging, I realized the issue was not in the code, but in the dataset: most views 
                  contained large backgrounds, very small objects, and almost no parallax. NeRF simply couldn‚Äôt 
                  recover meaningful geometry. To fix this, I reshot the entire dataset, keeping the object centered,
                  increasing the number of viewpoints, and making sure the camera moved around it in a full circle. 
                  After these adjustments, training became much better, though the outcome isn't satisfying enough.</p>
                <div class="result-box">
                  <div class="thumb large-box">
                      <img src="media/image.png" alt="Box filter result 1">
                      <div class="caption">my new dataset after realizing the problem</div>
                    </div>
                </div>
                <p>Here are the progress during training:</p>
                <div class="result-box">
                    <div class="thumb">
                      <img src="media/val_iter_1.png" alt="Box filter result 1">
                      <div class="caption">the first iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_250.png" alt="Box filter result 1">
                      <div class="caption">the 250th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_500.png" alt="Box filter result 1">
                      <div class="caption">the 500th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_1000.png" alt="Box filter result 1">
                      <div class="caption">the 1000th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_2750.png" alt="Box filter result 1">
                      <div class="caption">the 2750th iteration</div>
                    </div>
                    <div class="thumb">
                      <img src="media/val_iter_5000.png" alt="Box filter result 1">
                      <div class="caption">the 5000th iteration</div>
                    </div>
                  </div>
                <p>And here is the PSNR curve and Loss function:</p>
                <div class="result-box">
                  <div class="thumb large-box">
                      <img src="media/Traing_PSNRR.png" alt="Box filter result 1">
                      <div class="caption">the PSNR curve during trainig</div>
                    </div>
                  <div class="thumb large-box">
                      <img src="media/Training_loss.png" alt="Box filter result 1">
                      <div class="caption">the LOSS curve during trainig</div>
                    </div>
                </div>
                <p>Finally, here is the GIF of camera circling my own object</p>
                <div class="result-box">
                  <div class="thumb large-box">
                      <img src="media/output4.gif" alt="Box filter result 1">
                      <div class="caption">Spherical rendering video of my own dataset</div>
                    </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
    </section> 
  </main>
        
  <script>
    // Theme toggle with localStorage
    const root = document.documentElement;
    const key = 'pg-theme';
    const btn = document.getElementById('themeToggle');
    const saved = localStorage.getItem(key);
    if(saved){ document.documentElement.setAttribute('data-theme', saved); }
    btn.addEventListener('click', () => {
      const current = document.documentElement.getAttribute('data-theme');
      const next = current === 'light' ? 'dark' : 'light';
      document.documentElement.setAttribute('data-theme', next);
      localStorage.setItem(key, next);
    });
    // Tiny toggle for mini-box
    document.querySelectorAll('.mini-toggle').forEach(btn=>{
      btn.addEventListener('click', ()=>{
        const sel = btn.getAttribute('data-target');
        const box = document.querySelector(sel);
        const hidden = box.style.display === 'none';
        box.style.display = hidden ? '' : 'none';
        btn.textContent = hidden ? 'Hide' : 'Show';
      });
    });
  </script>
        
</body>
